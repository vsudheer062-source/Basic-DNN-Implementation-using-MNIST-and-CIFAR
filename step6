# Cell 6: Model Trainer
class ModelTrainer:
    """
    Comprehensive model training class
    """
    
    def __init__(self, model, train_loader, test_loader, device):
        """
        Initialize trainer
        
        Args:
            model: Neural network model
            train_loader: DataLoader for training data
            test_loader: DataLoader for test data
            device: torch.device for computation
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.device = device
        
        # Training history
        self.train_losses = []
        self.train_accuracies = []
        self.val_losses = []
        self.val_accuracies = []
    
    def train(self, num_epochs=10, learning_rate=0.001, optimizer_type='adam', 
              scheduler_type=None, save_best=True):
        """
        Train the model
        
        Args:
            num_epochs (int): Number of training epochs
            learning_rate (float): Learning rate
            optimizer_type (str): 'adam', 'sgd', or 'rmsprop'
            scheduler_type (str): 'step', 'cosine', or None
            save_best (bool): Whether to save the best model
        """
        
        # Setup criterion
        criterion = nn.CrossEntropyLoss()
        
        # Setup optimizer
        if optimizer_type.lower() == 'adam':
            optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        elif optimizer_type.lower() == 'sgd':
            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, 
                                momentum=0.9, weight_decay=1e-4)
        elif optimizer_type.lower() == 'rmsprop':
            optimizer = optim.RMSprop(self.model.parameters(), lr=learning_rate)
        else:
            raise ValueError("Optimizer must be 'adam', 'sgd', or 'rmsprop'")
        
        # Setup scheduler
        scheduler = None
        if scheduler_type == 'step':
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
        elif scheduler_type == 'cosine':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
        
        best_accuracy = 0.0
        start_time = time.time()
        
        print(f"Training {self.model.__class__.__name__} for {num_epochs} epochs...")
        print(f"Device: {self.device}")
        print(f"Optimizer: {optimizer_type}")
        print(f"Learning Rate: {learning_rate}")
        print("-" * 50)
        
        for epoch in range(num_epochs):
            # Training phase
            train_loss, train_acc = self._train_epoch(criterion, optimizer)
            
            # Validation phase
            val_loss, val_acc = self._validate_epoch(criterion)
            
            # Update learning rate
            if scheduler:
                scheduler.step()
            
            # Store history
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_acc)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            
            # Print epoch results
            print(f'Epoch [{epoch+1}/{num_epochs}]')
            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            if scheduler:
                print(f'LR: {optimizer.param_groups[0]["lr"]:.6f}')
            print('-' * 30)
            
            # Save best model
            if save_best and val_acc > best_accuracy:
                best_accuracy = val_acc
                torch.save(self.model.state_dict(), 'best_model.pth')
                print(f'âœ“ New best model saved with accuracy: {best_accuracy:.2f}%')
        
        total_time = time.time() - start_time
        print(f"\nTraining completed in {total_time//60:.0f}m {total_time%60:.0f}s")
        print(f"Best validation accuracy: {best_accuracy:.2f}%")
        
        return {
            'train_losses': self.train_losses,
            'train_accuracies': self.train_accuracies,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'best_accuracy': best_accuracy
        }
    
    def _train_epoch(self, criterion, optimizer):
        """Train for one epoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        train_pbar = tqdm(self.train_loader, desc='Training', leave=False)
        
        for batch_idx, (data, target) in enumerate(train_pbar):
            data, target = data.to(self.device), target.to(self.device)
            
            # Zero gradients
            optimizer.zero_grad()
            
            # Forward pass
            output = self.model(data)
            loss = criterion(output, target)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            # Statistics
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # Update progress bar
            train_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        epoch_loss = running_loss / len(self.train_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def _validate_epoch(self, criterion):
        """Validate for one epoch"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                output = self.model(data)
                loss = criterion(output, target)
                
                running_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        epoch_loss = running_loss / len(self.test_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc

print("ModelTrainer class defined successfully!")
