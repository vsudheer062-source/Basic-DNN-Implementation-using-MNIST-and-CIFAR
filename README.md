Here's a complete guide to run the Deep Neural Network project in Google Colab:

# Cell 13: Save Results and Generate Report
def generate_final_report():
    """
    Generate a comprehensive final report
    """
    print(f"\n{'='*60}")
    print("FINAL PROJECT REPORT")
    print(f"{'='*60}")

    report = f"""
DEEP NEURAL NETWORK PROJECT - COMPREHENSIVE ANALYSIS
==================================================

1. EXPERIMENT OVERVIEW:
   - Datasets: MNIST (handwritten digits) and CIFAR-10 (natural images)
   - Models: Basic DNN for MNIST, CNN for CIFAR-10
   - Training: Adam optimizer, learning rate scheduling, early stopping

2. RESULTS SUMMARY:

   MNIST (Basic DNN):
   - Final Test Accuracy: {mnist_results['accuracy']:.2f}%
   - Final Test Loss: {mnist_results['loss']:.4f}
   - Model Parameters: {sum(p.numel() for p in mnist_model.parameters()):,}
   - Best Validation Accuracy: {max(mnist_history['val_accuracies']):.2f}%

   CIFAR-10 (CNN):
   - Final Test Accuracy: {cifar10_results['accuracy']:.2f}%
   - Final Test Loss: {cifar10_results['loss']:.4f}
   - Model Parameters: {sum(p.numel() for p in cifar10_model.parameters()):,}
   - Best Validation Accuracy: {max(cifar10_history['val_accuracies']):.2f}%

3. KEY FINDINGS:
   - CNN architecture is more suitable for complex image classification tasks
   - Proper data augmentation and normalization improve generalization
   - Learning rate scheduling helps in achieving better convergence
   - Batch normalization and dropout significantly reduce overfitting

4. MODEL PERFORMANCE:
   - Both models achieved good performance on their respective datasets
   - MNIST being a simpler dataset achieved higher accuracy with a basic DNN
   - CIFAR-10 required more sophisticated CNN architecture for good performance

5. TECHNICAL SPECIFICATIONS:
   - Framework: PyTorch
   - Training Environment: Google Colab
   - Hardware: {'GPU' if torch.cuda.is_available() else 'CPU'}
   - Training Time: Approximately {'15 minutes' if torch.cuda.is_available() else '45 minutes'}

6. FUTURE IMPROVEMENTS:
   - Implement more advanced architectures (ResNet, EfficientNet)
   - Add more data augmentation techniques
   - Experiment with different loss functions
   - Implement ensemble methods for better performance
"""

    print(report)

    # Save models if needed
    print("\n7. SAVING MODELS:")
    try:
        torch.save(mnist_model.state_dict(), 'mnist_best_model.pth')
        print("âœ“ MNIST model saved as 'mnist_best_model.pth'")

        torch.save(cifar10_model.state_dict(), 'cifar10_best_model.pth')
        print("âœ“ CIFAR-10 model saved as 'cifar10_best_model.pth'")

        # Save training history
        import json
        history_data = {
            'mnist': {
                'train_losses': mnist_history['train_losses'],
                'train_accuracies': mnist_history['train_accuracies'],
                'val_losses': mnist_history['val_losses'],
                'val_accuracies': mnist_history['val_accuracies']
            },
            'cifar10': {
                'train_losses': cifar10_history['train_losses'],
                'train_accuracies': cifar10_history['train_accuracies'],
                'val_losses': cifar10_history['val_losses'],
                'val_accuracies': cifar10_history['val_accuracies']
            }
        }

        with open('training_history.json', 'w') as f:
            json.dump(history_data, f)
        print("âœ“ Training history saved as 'training_history.json'")

    except Exception as e:
        print(f"âš  Error saving models: {e}")

    print(f"\n{'='*60}")
    print("PROJECT COMPLETED SUCCESSFULLY! ðŸŽ‰")
    print(f"{'='*60}")

# Generate final report
generate_final_report()
